{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Transformer Encoder with MLM Objective on BERT-style tokens / wikitext dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup (retrieve data, look at `mask_dataset_for_mlm` for important MLM preprocessing steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishubtamirisa/miniforge3/envs/LM-impl/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-103-v1 to /Users/rishubtamirisa/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 190M/190M [00:48<00:00, 3.89MB/s] \n",
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to /Users/rishubtamirisa/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 15.95it/s]\n",
      " 40%|████      | 2/5 [00:00<00:00, 16.84ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 5/5 [00:00<00:00, 19.20ba/s]\n",
      "100%|██████████| 1802/1802 [01:32<00:00, 19.39ba/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 18.74ba/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.43ba/s]\n",
      "100%|██████████| 1802/1802 [14:47<00:00,  2.03ba/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.27ba/s]\n"
     ]
    }
   ],
   "source": [
    "from modules.encoder import EncoderModel\n",
    "from preprocess.mlm_preprocess import get_dataset_example, mask_dataset_for_mlm\n",
    "\n",
    "input_ids, tokenizer = get_dataset_example()\n",
    "mlm_input_ids, mlm_labels = mask_dataset_for_mlm(input_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderModel(\n",
       "  (embedding): Embedding(30522, 512)\n",
       "  (pos_en): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): Encoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_proj): Linear(in_features=512, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from modules.encoder import EncoderModel\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dim = 512\n",
    "model_dim = 512\n",
    "n_layers = 6\n",
    "num_heads = 8\n",
    "encoder = EncoderModel(vocab_size=vocab_size, embed_dim=embed_dim, model_dim=model_dim, n_layers=n_layers, num_heads=num_heads)\n",
    "encoder.to(\"mps\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset( mlm_input_ids, mlm_labels )\n",
    "loader = DataLoader(dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_mlm(epochs, model, tokenizer, loader, optimizer=torch.optim.Adam, device=torch.device('cpu')):\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for _ in range(epochs):\n",
    "            cur_batch = 0\n",
    "            total_batches = len(loader) \n",
    "            for batch in loader:\n",
    "                input_ids, labels = batch\n",
    "                input_ids = input_ids.to(device, dtype=torch.int64)\n",
    "                labels = labels.to(device, dtype=torch.int64)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(input_ids)\n",
    "                loss = criterion(output.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                cur_batch += 1\n",
    "                pbar.set_postfix(**{\"batch: \": f\"{cur_batch} / {total_batches}\", \"loss:\": loss.item()})\n",
    "        \n",
    "        checkpoint = {'vocab_size': tokenizer.vocab_size,\n",
    "                      'embed_dim': embed_dim,\n",
    "                      'model_dim': model_dim,\n",
    "                      'n_layers': n_layers,\n",
    "                      'num_heads': num_heads,\n",
    "                      'state_dict': model.state_dict()}\n",
    "        torch.save(checkpoint, 'model_checkpoints/checkpoint.pth')\n",
    "\n",
    "train_mlm(epochs=4, tokenizer=tokenizer, model=encoder, loader=loader, optimizer=torch.optim.Adam(encoder.parameters(), lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of parameters in model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder):,} trainable parameters')\n",
    "\n",
    "# print model architecture\n",
    "print(encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI love [MASK] and [MASK] .\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([tokenizer\u001b[39m.\u001b[39mencode(text)])\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[39m=\u001b[39m encoder(input_ids) \u001b[39m# (batch_size, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m predicted_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(torch\u001b[39m.\u001b[39msoftmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m predicted_token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(predicted_index[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1600\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1834\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1373\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    model = EncoderModel(vocab_size=checkpoint['vocab_size'], \n",
    "                         embed_dim=checkpoint['embed_dim'], \n",
    "                         model_dim=checkpoint['model_dim'], \n",
    "                         n_layers=checkpoint['n_layers'], \n",
    "                         num_heads=checkpoint['num_heads'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model\n",
    "\n",
    "# load model from checkpoint\n",
    "encoder = load_model_from_checkpoint('model_checkpoints/checkpoint2.pth')\n",
    "\n",
    "# test model on input text with masked tokens\n",
    "text = \"I love [MASK] and [MASK] .\"\n",
    "input_ids = torch.tensor([tokenizer.encode(text)])\n",
    "\n",
    "outputs = encoder(input_ids) # (batch_size, seq_len, vocab_size)\n",
    "predicted_index = torch.argmax(torch.softmax(outputs, dim=-1), dim=-1)\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_index[0].tolist())\n",
    "\n",
    "print(predicted_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thought. families [MASK] [MASK] deities, with [MASK] father, [MASK], and child, represent the creation [MASK] new life and [MASK] succession of the father [MASK] the child, [MASK] pattern [MASK] connects divine families with royal [MASK]. osiri [MASK], isis, and horus formed the quintessential family of this type. the pattern they set grew [MASK] widespread over time, so that many deities in local cult centers, like ptah, sekhmet, and their child [MASK]fer [MASK] at memphis and amun [MASK] mut, and khon [MASK] at [MASK]bes, were assembled into family triads. genealogical connections like these are changeable, in'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode first row of mlm_input_ids\n",
    "tokenizer.decode(mlm_input_ids[300].int())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LM-impl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbbfbac07cdef3f27747317668d6c981de600c86bba094be9fa5a36a0eb181eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
