{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Transformer Encoder with MLM Objective on BERT-style tokens / wikitext dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup (retrieve data, look at `mask_dataset_for_mlm` for important MLM preprocessing steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.encoder import EncoderModel\n",
    "from preprocess.mlm_preprocess import get_dataset_example, mask_dataset_for_mlm\n",
    "\n",
    "dataset, tokenizer = get_dataset_example()\n",
    "dataset = mask_dataset_for_mlm(dataset,vocab_size=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode entry at index 4 of mlm_input_ids\n",
    "# tokenizer.decode(mlm_input_ids[3875])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules.encoder import EncoderModel, Encoder\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dim = 512\n",
    "model_dim = 512\n",
    "n_layers = 6\n",
    "num_heads = 8\n",
    "encoder = EncoderModel(vocab_size=vocab_size, embed_dim=embed_dim, model_dim=model_dim, n_layers=n_layers, num_heads=num_heads)\n",
    "encoder.to(\"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset( dataset['input_ids'], dataset['labels'], dataset['attention_mask'] )\n",
    "loader = DataLoader(dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "def train_mlm(epochs, model, vocab_size, loader, optimizer=torch.optim.Adam, device=torch.device('cpu'), wandb_log=True):\n",
    "    if wandb_log:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"encoder-mlm\",\n",
    "            \n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"learning_rate\": optimizer.defaults['lr'],\n",
    "            \"architecture\": \"Transformer\",\n",
    "            \"dataset\": \"wikitext-2\",\n",
    "            \"epochs\": epochs,\n",
    "            }\n",
    "        )\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            cur_batch = 0\n",
    "            total_batches = len(loader) \n",
    "            for batch in loader:\n",
    "                input_ids, labels, attention_mask = batch\n",
    "                input_ids = input_ids.to(device, dtype=torch.int64)\n",
    "                labels = labels.to(device, dtype=torch.int64)\n",
    "                output = model(input_ids, attention_mask.unsqueeze(1).to(device))\n",
    "                loss = criterion(output.view(-1, vocab_size), labels.view(-1))\n",
    "                if wandb_log:\n",
    "                    wandb.log({\"train_loss\": loss.item()}, step=cur_batch + (epoch * total_batches))  # Log the loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.zero_grad()\n",
    "                cur_batch += 1\n",
    "                pbar.set_postfix(**{f\"Epoch {epoch} / batch: \": f\"{cur_batch} / {total_batches}\", \"loss:\": loss.item()})\n",
    "        \n",
    "        checkpoint = {'vocab_size': tokenizer.vocab_size,\n",
    "                      'embed_dim': embed_dim,\n",
    "                      'model_dim': model_dim,\n",
    "                      'n_layers': n_layers,\n",
    "                      'num_heads': num_heads,\n",
    "                      'state_dict': model.state_dict()}\n",
    "        torch.save(checkpoint, f'model_checkpoints/checkpoint_E{epoch}.pth')\n",
    "        if wandb_log:\n",
    "            wandb.save(f'model_checkpoints/checkpoint_E{epoch}.pth')  # Save the model checkpoint to wandb\n",
    "\n",
    "train_mlm(epochs=4, \n",
    "          vocab_size=vocab_size, \n",
    "          model=encoder, \n",
    "          loader=loader, \n",
    "          optimizer=torch.optim.AdamW(encoder.parameters(), lr=1e-4),\n",
    "          wandb_log=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of parameters in model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder):,} trainable parameters')\n",
    "\n",
    "# print model architecture\n",
    "print(encoder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    model = EncoderModel(vocab_size=checkpoint['vocab_size'], \n",
    "                         embed_dim=checkpoint['embed_dim'], \n",
    "                         model_dim=checkpoint['model_dim'], \n",
    "                         n_layers=checkpoint['n_layers'], \n",
    "                         num_heads=checkpoint['num_heads'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model\n",
    "\n",
    "def predict_masked_sent(model, tokenizer, text, top_k=5):\n",
    "    '''\n",
    "    Masked token inference. Credit: https://gist.github.com/yuchenlin/a2f42d3c4378ed7b83de65c7a2222eb2\n",
    "    '''\n",
    "    # Tokenize input\n",
    "    text = \"[CLS] %s [SEP]\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    masked_index = tokenized_text.index(\"[MASK]\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(outputs, dim=-1)[0, masked_index]\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "\n",
    "model = load_model_from_checkpoint('model_checkpoints/checkpoint.pth')\n",
    "text = \" I love [MASK] .\"\n",
    "predict_masked_sent(model, tokenizer=tokenizer, text=text, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LM-impl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbbfbac07cdef3f27747317668d6c981de600c86bba094be9fa5a36a0eb181eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
