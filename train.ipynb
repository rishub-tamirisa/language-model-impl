{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Transformer Encoder with MLM Objective on BERT-style tokens / wikitext dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup (retrieve data, look at `mask_dataset_for_mlm` for important MLM preprocessing steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishubtamirisa/miniforge3/envs/LM-impl/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset wikitext (/Users/rishubtamirisa/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 237.08it/s]\n",
      "Loading cached processed dataset at /Users/rishubtamirisa/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f23794ddde0ffc01.arrow\n",
      "Loading cached processed dataset at /Users/rishubtamirisa/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-aeb8e5d15c439c91.arrow\n",
      "100%|██████████| 4/4 [00:00<00:00, 19.74ba/s]\n"
     ]
    }
   ],
   "source": [
    "from modules.encoder import EncoderModel\n",
    "from preprocess.mlm_preprocess import get_dataset_example, mask_dataset_for_mlm\n",
    "\n",
    "input_ids, tokenizer = get_dataset_example()\n",
    "mlm_input_ids, mlm_labels = mask_dataset_for_mlm(input_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderModel(\n",
       "  (embedding): Embedding(30522, 512)\n",
       "  (pos_en): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): Encoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): EncoderBlock(\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (qkv_weights_list): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (1): Linear(in_features=512, out_features=64, bias=True)\n",
       "              (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (W_ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (W_ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_proj): Linear(in_features=512, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from modules.encoder import EncoderModel, Encoder\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dim = 512\n",
    "model_dim = 512\n",
    "n_layers = 6\n",
    "num_heads = 8\n",
    "encoder = EncoderModel(vocab_size=vocab_size, embed_dim=embed_dim, model_dim=model_dim, n_layers=n_layers, num_heads=num_heads)\n",
    "encoder.to(\"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset( mlm_input_ids, mlm_labels )\n",
    "loader = DataLoader(dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrishubtamirisa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rishubtamirisa/UIUC/NNProjects/transformer/language-model-impl/wandb/run-20230126_234758-0dg5y2us</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rishubtamirisa/encoder-mlm/runs/0dg5y2us\" target=\"_blank\">sweet-moon-2</a></strong> to <a href=\"https://wandb.ai/rishubtamirisa/encoder-mlm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/rishubtamirisa/encoder-mlm\" target=\"_blank\">https://wandb.ai/rishubtamirisa/encoder-mlm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/rishubtamirisa/encoder-mlm/runs/0dg5y2us\" target=\"_blank\">https://wandb.ai/rishubtamirisa/encoder-mlm/runs/0dg5y2us</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [08:35<?, ?it/s, Epoch 0 / batch: =125 / 743, loss:=7.39]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         torch\u001b[39m.\u001b[39msave(checkpoint, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel_checkpoints/checkpoint_E\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m         wandb\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel_checkpoints/checkpoint_E\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Save the model checkpoint to wandb\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m train_mlm(epochs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, \n\u001b[1;32m     48\u001b[0m           tokenizer\u001b[39m=\u001b[39;49mtokenizer, \n\u001b[1;32m     49\u001b[0m           model\u001b[39m=\u001b[39;49mencoder, \n\u001b[1;32m     50\u001b[0m           loader\u001b[39m=\u001b[39;49mloader, \n\u001b[1;32m     51\u001b[0m           optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(encoder\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m5e-5\u001b[39;49m)\n\u001b[1;32m     52\u001b[0m           )\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mtrain_mlm\u001b[0;34m(epochs, model, tokenizer, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m     27\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[0;32m---> 28\u001b[0m output \u001b[39m=\u001b[39m model(input_ids)\n\u001b[1;32m     29\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, tokenizer\u001b[39m.\u001b[39mvocab_size), labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     30\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m: loss\u001b[39m.\u001b[39mitem()}, step\u001b[39m=\u001b[39mcur_batch \u001b[39m+\u001b[39m (epoch \u001b[39m*\u001b[39m total_batches))  \u001b[39m# Log the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/UIUC/NNProjects/transformer/language-model-impl/modules/encoder.py:32\u001b[0m, in \u001b[0;36mEncoderModel.forward\u001b[0;34m(self, input_ids, input_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m\u001b[39m\u001b[39m''' \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mEmbedding permutes are to make the embedding dimension the first dimension for positional encoding (just for compatability with PyTorch code in positional_encoding.py). This is still just embedding + positional encoding\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     31\u001b[0m input_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embedding \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_en(embedding\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(input_embedding, input_mask)\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(X)\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/UIUC/NNProjects/transformer/language-model-impl/modules/encoder.py:45\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input_embedding, input_mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m X \u001b[39m=\u001b[39m input_embedding\n\u001b[1;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers:\n\u001b[0;32m---> 45\u001b[0m     X \u001b[39m=\u001b[39m layer(X, input_mask)\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/UIUC/NNProjects/transformer/language-model-impl/modules/encoder.py:67\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, input_embeddings, input_mask)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_embeddings, input_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     65\u001b[0m     \u001b[39m# compute self attention\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     X \u001b[39m=\u001b[39m input_embeddings\n\u001b[0;32m---> 67\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_head_attention(X, X, X, input_mask) \u001b[39m+\u001b[39m X) \u001b[39m# add residual connection + layer_norm\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[39m# compute feed-forward\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(X) \u001b[39m+\u001b[39m X) \u001b[39m# add residual connection + layer_norm\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/UIUC/NNProjects/transformer/language-model-impl/modules/attention.py:52\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m# Shape(out) -> (seq_len, (model_dim * num_heads))\u001b[39;00m\n\u001b[1;32m     51\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(out)\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(out)\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniforge3/envs/LM-impl/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "def train_mlm(epochs, model, tokenizer, loader, optimizer=torch.optim.Adam, device=torch.device('cpu')):\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"transformer-mlm\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"learning_rate\": optimizer.defaults['lr'],\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"wikitext-2\",\n",
    "        \"epochs\": epochs,\n",
    "        }\n",
    "    )\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            cur_batch = 0\n",
    "            total_batches = len(loader) \n",
    "            for batch in loader:\n",
    "                input_ids, labels = batch\n",
    "                input_ids = input_ids.to(device, dtype=torch.int64)\n",
    "                labels = labels.to(device, dtype=torch.int64)\n",
    "                output = model(input_ids)\n",
    "                loss = criterion(output.view(-1, vocab_size), labels.view(-1))\n",
    "                wandb.log({\"train_loss\": loss.item()}, step=cur_batch + (epoch * total_batches))  # Log the loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.zero_grad()\n",
    "                cur_batch += 1\n",
    "                pbar.set_postfix(**{f\"Epoch {epoch} / batch: \": f\"{cur_batch} / {total_batches}\", \"loss:\": loss.item()})\n",
    "        \n",
    "        checkpoint = {'vocab_size': vocab_size,\n",
    "                      'embed_dim': embed_dim,\n",
    "                      'model_dim': model_dim,\n",
    "                      'n_layers': n_layers,\n",
    "                      'num_heads': num_heads,\n",
    "                      'state_dict': model.state_dict()}\n",
    "        torch.save(checkpoint, f'model_checkpoints/checkpoint_E{epoch}.pth')\n",
    "        wandb.save(f'model_checkpoints/checkpoint_E{epoch}.pth')  # Save the model checkpoint to wandb\n",
    "\n",
    "train_mlm(epochs=4, \n",
    "          tokenizer=tokenizer, \n",
    "          model=encoder, \n",
    "          loader=loader, \n",
    "          optimizer=torch.optim.Adam(encoder.parameters(), lr=5e-5)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of parameters in model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder):,} trainable parameters')\n",
    "\n",
    "# print model architecture\n",
    "print(encoder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    model = EncoderModel(vocab_size=checkpoint['vocab_size'], \n",
    "                         embed_dim=checkpoint['embed_dim'], \n",
    "                         model_dim=checkpoint['model_dim'], \n",
    "                         n_layers=checkpoint['n_layers'], \n",
    "                         num_heads=checkpoint['num_heads'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model\n",
    "\n",
    "def predict_masked_sent(model, tokenizer, text, top_k=5):\n",
    "    '''\n",
    "    Masked token inference. Credit: https://gist.github.com/yuchenlin/a2f42d3c4378ed7b83de65c7a2222eb2\n",
    "    '''\n",
    "    # Tokenize input\n",
    "    text = \"[CLS] %s [SEP]\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    masked_index = tokenized_text.index(\"[MASK]\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(outputs, dim=-1)[0, masked_index]\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "\n",
    "model = load_model_from_checkpoint('model_checkpoints/checkpoint.pth')\n",
    "text = \" I love [MASK] .\"\n",
    "predict_masked_sent(model, tokenizer=tokenizer, text=text, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LM-impl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbbfbac07cdef3f27747317668d6c981de600c86bba094be9fa5a36a0eb181eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
