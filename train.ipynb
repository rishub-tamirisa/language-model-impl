{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Transformer Encoder with MLM Objective on BERT-style tokens / wikitext dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup (retrieve data, look at `mask_dataset_for_mlm` for important MLM preprocessing steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.encoder import EncoderModel\n",
    "from preprocess.mlm_preprocess import get_dataset_example, mask_dataset_for_mlm\n",
    "\n",
    "input_ids, tokenizer = get_dataset_example()\n",
    "mlm_input_ids, mlm_labels = mask_dataset_for_mlm(input_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules.encoder import EncoderModel\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dim = 512\n",
    "model_dim = 512\n",
    "n_layers = 6\n",
    "num_heads = 8\n",
    "encoder = EncoderModel(vocab_size=vocab_size, embed_dim=embed_dim, model_dim=model_dim, n_layers=n_layers, num_heads=num_heads)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset( mlm_input_ids, mlm_labels )\n",
    "loader = DataLoader(dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_mlm(epochs, model, tokenizer, loader, optimizer=torch.optim.Adam, device=torch.device('cpu')):\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for _ in range(epochs):\n",
    "            for batch in loader:\n",
    "                input_ids, labels = batch\n",
    "                input_ids = input_ids.to(device, dtype=torch.int64)\n",
    "                labels = labels.to(device, dtype=torch.int64)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(input_ids)\n",
    "                loss = criterion(output.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.set_postfix(\"loss\", loss.item())\n",
    "\n",
    "train_mlm(model=encoder, loader=loader, optimizer=torch.optim.Adam(encoder.parameters(), lr=1e-4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LM-impl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbbfbac07cdef3f27747317668d6c981de600c86bba094be9fa5a36a0eb181eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
